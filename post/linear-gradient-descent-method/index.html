<!DOCTYPE html>
<html lang="en">
  <head>
    
    
      <title>
        以梯度下降法解線性迴歸係數 ::
        Hello Friend, I&#39;m Yesting!
      </title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta
  name="description"
  content="程式說明此篇文章說明如何用梯度下降(Gradient Descent)方法找出線性迴歸最佳參數解。實作參考自國立台灣大學資訊網路與多媒體研究所李宏毅老師的Youtube教學影片：
ML Lecture 1: Regression - Case Study
ML Lecture 1: Regression - Demo
使用套件tidyverse為R軟體專為資料科學的集合套件，裡面包含常用的dplyr及ggplot2等常用的資料科學套件。詳情資訊可至tidyverse套件官網查看。
library(tidyverse)使用資料集及分析問題本次資料集使用ggplot2套件內提供的鑽石(diamonds)資料集，裡面共包含53,940筆及10個特徵資料。分析的問題為給定在切割(cut)為理想型(Ideal)及淨度(clarity)為內部完美無瑕(IF)的鑽石資料集下，以克拉做為特徵資料(即解釋變數)，建立線性迴歸模型，預測鑽石價格(即被解釋變數)。有關鑽石的知識，可參考此網站。
預測的迴歸式如下，下標i代表樣本：
\[Price_i=\hat{\alpha} &#43; \hat{\beta_1} Carat_i &#43; \varepsilon_i\]
資料整理程式碼如下所示：
data &amp;lt;- diamonds %&amp;gt;% filter((cut==&amp;quot;Ideal&amp;quot;) &amp;amp; (clarity==&amp;quot;IF&amp;quot;)) %&amp;gt;%select(price, carat)data## # A tibble: 1,212 x 2## price carat## &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;## 1 2783 0.52## 2 2789 0.55## 3 2790 0."
/>
<meta
  name="keywords"
  content=""
/>
<meta name="robots" content="noodp" />
<link rel="canonical" href="/post/linear-gradient-descent-method/" />


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'G-L50G2LV7Q6', 'auto');
	
	ga('send', 'pageview');
}
</script>






<link rel="stylesheet" href="/assets/style.css" />

<link rel="stylesheet" href="/style.css" />


<link
  rel="apple-touch-icon-precomposed"
  sizes="144x144"
  href="/img/apple-touch-icon-144-precomposed.png"
/>
<link rel="shortcut icon" href="/img/favicon.png" />


<link href="/assets/fonts/Inter-Italic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="/assets/fonts/Inter-Regular.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="/assets/fonts/Inter-Medium.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="/assets/fonts/Inter-MediumItalic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="/assets/fonts/Inter-Bold.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="/assets/fonts/Inter-BoldItalic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="以梯度下降法解線性迴歸係數"/>
<meta name="twitter:description" content="此篇文章說明如何用梯度下降(Gradient Descent)方法找出線性迴歸最佳參數解。"/>



<meta property="og:title" content="以梯度下降法解線性迴歸係數" />
<meta property="og:description" content="此篇文章說明如何用梯度下降(Gradient Descent)方法找出線性迴歸最佳參數解。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/linear-gradient-descent-method/" />
<meta property="article:published_time" content="2018-09-17T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-09-17T00:00:00+00:00" /><meta property="og:site_name" content="Hello Friend, I&#39;m Yesting!" />







    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-L50G2LV7Q6"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-L50G2LV7Q6');
    </script>

  </head>
  <body class="dark-theme">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a
  href="/"
  class="logo"
  style="text-decoration: none;"
>
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367" />
</svg>
</span>
    <span class="logo__text"
      >Hello Friend, I&#39;m Yestin!</span
    >
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
      
      
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none" />
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z" />
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg
  class="theme-toggler"
  width="24"
  height="24"
  viewBox="0 0 48 48"
  fill="none"
  xmlns="http://www.w3.org/2000/svg"
>
  <path
    d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"
  />
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  <div class="post">
    <h1 class="post-title">以梯度下降法解線性迴歸係數</h1>
    <div class="post-meta">
      
        <span class="post-date">
          2018-09-17
        </span>

        
          
        
      

      
        <span class="post-author"
          >— Written by Yen-Ting, Su</span
        >


      
    </div>

    
      <span class="post-tags">
        
          <a href="/tags/r/">#R</a>&nbsp;
        
          <a href="/tags/machine-learning/">#Machine Learning</a>&nbsp;
        
      </span>
    

    

    <div class="post-content">
      
      


<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<hr />
<div id="程式說明" class="section level2">
<h2>程式說明</h2>
<p>此篇文章說明如何用梯度下降(Gradient Descent)方法找出線性迴歸最佳參數解。實作參考自國立台灣大學資訊網路與多媒體研究所<strong>李宏毅</strong>老師的Youtube教學影片：</p>
<ol style="list-style-type: decimal">
<li><p><a href="https://www.youtube.com/watch?v=fegAeph9UaA&amp;index=3&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49">ML Lecture 1: Regression - Case Study</a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=1UqCjFQiiy0&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=4">ML Lecture 1: Regression - Demo</a></p></li>
</ol>
<hr />
</div>
<div id="使用套件" class="section level2">
<h2>使用套件</h2>
<p><code>tidyverse</code>為R軟體專為資料科學的集合套件，裡面包含常用的<code>dplyr</code>及<code>ggplot2</code>等常用的資料科學套件。詳情資訊可至<a href="https://www.tidyverse.org/">tidyverse套件官網</a>查看。</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<hr />
</div>
<div id="使用資料集及分析問題" class="section level2">
<h2>使用資料集及分析問題</h2>
<p>本次資料集使用<code>ggplot2</code>套件內提供的鑽石(<code>diamonds</code>)資料集，裡面共包含53,940筆及10個特徵資料。分析的問題為給定在切割(cut)為理想型(Ideal)及淨度(clarity)為內部完美無瑕(IF)的鑽石資料集下，以克拉做為特徵資料(即解釋變數)，建立線性迴歸模型，預測鑽石價格(即被解釋變數)。有關鑽石的知識，可參考此<a href="https://www.alren.com.hk/CHT/CMS/Diamond-Education/14">網站</a>。</p>
<p>預測的迴歸式如下，下標<code>i</code>代表樣本：</p>
<p><span class="math display">\[Price_i=\hat{\alpha} + \hat{\beta_1} Carat_i + \varepsilon_i\]</span></p>
<p>資料整理程式碼如下所示：</p>
<pre class="r"><code>data &lt;- diamonds %&gt;% 
  filter((cut==&quot;Ideal&quot;) &amp; (clarity==&quot;IF&quot;)) %&gt;%
  select(price, carat)
data</code></pre>
<pre><code>## # A tibble: 1,212 x 2
##    price carat
##    &lt;int&gt; &lt;dbl&gt;
##  1  2783  0.52
##  2  2789  0.55
##  3  2790  0.64
##  4  2800  0.61
##  5  2802  0.53
##  6  2868  0.62
##  7  2869  0.62
##  8  2925  0.71
##  9  2952  0.74
## 10  2960  0.64
## # ... with 1,202 more rows</code></pre>
<p>經過<code>filter</code>挑選切割(cut)為理想型(Ideal)及淨度(clarity)為內部完美無瑕(IF)的鑽石資料集後，只剩下1,212筆資料。</p>
<hr />
</div>
<div id="迴歸函數與封閉解法" class="section level2">
<h2>迴歸函數與封閉解法</h2>
<p>我們直接透過R軟體的<code>lm</code>函數，直接跑線性迴歸模型，估計出參數值。</p>
<pre class="r"><code>model &lt;- lm(price ~ carat, data = data)
model</code></pre>
<pre><code>## 
## Call:
## lm(formula = price ~ carat, data = data)
## 
## Coefficients:
## (Intercept)        carat  
##       -3043        11682</code></pre>
<p>估計出來的模型的<span class="math inline">\(\hat{\alpha}=-3403\)</span>，<span class="math inline">\(\hat{\beta_1}=11682\)</span>。</p>
<p>繪製圖形觀看迴歸線配適狀況:</p>
<pre class="r"><code>ggplot(data, aes(x = carat, y = price)) + 
  geom_point() +
  stat_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-09-19-linear-gradient-descent-method_files/figure-html/diamonds%20reg%20graph-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>除透過程式的函數外，也可直接以線性迴歸的封閉解(Closed-form solution)進行計算：</p>
<p><span class="math display">\[\beta=(X^TX)^{-1}X^TY\]</span></p>
<pre class="r"><code># 整理解釋變數資料並轉為matrix格式
x &lt;- data %&gt;% 
  transmute(Intercept = 1, carat) %&gt;%
  as.matrix()

# 整理被解釋變數資料並轉為vector格式
y &lt;- data %&gt;% pull(price)

# 封閉解
beta &lt;- (solve(t(x) %*% x) %*% t(x) %*% y)

beta</code></pre>
<pre><code>##               [,1]
## Intercept -3042.77
## carat     11681.76</code></pre>
<p>可以發現封閉解解法和套件執行的結果是一樣的。</p>
<hr />
</div>
<div id="梯度下降法" class="section level2">
<h2>梯度下降法</h2>
<p>為了有「機器學習」的感覺，此處示範以梯度下降法的方式，來找出線性迴歸最佳的參數。</p>
<p>在線性迴歸模型中，誤差函數(Error Function)為均方誤差(MSE, Mean squared error)：</p>
<p><span class="math display">\[ L(\hat{\alpha}, \hat{\beta_1}) = \frac{1}{N} \sum_{n=1}^{N} (y_{i}-\hat{y_{i})}^{2}=\frac{1}{N} \sum_{n=1}^{N}(y_{i} - \hat{\alpha} - \hat{\beta_1} Carat_i)^{2}\]</span></p>
<p>接下來將誤差函數對<span class="math inline">\(\hat{\alpha}\)</span>及<span class="math inline">\(\hat{\beta_1}\)</span>進行偏微分，得到梯度下降的計算式：</p>
<p><span class="math display">\[\frac{\partial L}{\partial \hat{\alpha}} = \frac{1}{N} \sum_{n=1}^{N} 2(y_{i} - \hat{\alpha} - \hat{\beta_1} Carat_i)(-1)\]</span></p>
<p><span class="math display">\[\frac{\partial L}{\partial \hat{\beta_1}} = \frac{1}{N} \sum_{n=1}^{N} 2(y_{i} - \hat{\alpha} - \hat{\beta_1} Carat_i)(-Carat_i)\]</span></p>
<p>每次疊代時，參數依據學習比率乘上梯度值進行調整。</p>
<pre class="r"><code># 設定梯度下降參數
lr &lt;- 0.1                 # 學習比率
maxIterations &lt;- 20000    # 最大疊代次數
y &lt;- data$price           # 被解釋變數資料
x &lt;- data$carat           # 解釋變數資料
alpha &lt;- 0                # 截距項初始值
beta &lt;- 0                 # 解釋變數係數初始值
iterations &lt;- 1           # 疊代次數起始值
traningRecord &lt;- tibble(alpha, beta)   # 儲存訓練過程資訊

while(iterations &lt;= maxIterations){
  
  # 以目前的截距項及特徵係數值進行預測
  yPred &lt;- alpha + beta * x
  
  # 損失函數對截距項進行偏微分
  alphaGrad &lt;- sum(2 * (y - yPred) * (-1)) / nrow(data)  
  
  # 損失函數對特徵係數進行偏微分
  betaGrad &lt;- sum(2 * (y - yPred) * (-x)) / nrow(data)   
  
  # 提前終止條件
  if((abs(alphaGrad)&lt;=(10^(-5))) &amp; all(abs(betaGrad)&lt;=(10^(-5)))){
    
    cat(&quot;在第&quot; ,iterations,&quot;次 疊代已達收斂條件\n&quot;)
    break
    
  }else if((abs(alphaGrad)&gt;=(10^(10))) &amp; all(abs(betaGrad)&gt;=(10^(10)))){
    
    cat(&quot;在第&quot; ,iterations,&quot;次 確認數值發散，終止運作\n&quot;)
    break
  }
  
  # 調整截距項及特徵係數值
  alpha &lt;- alpha - lr * alphaGrad
  beta &lt;- beta - lr * betaGrad
  
  # 記錄疊代次數
  iterations &lt;- iterations + 1    
  
  # 記錄訓練資訊
  traningRecord &lt;- traningRecord %&gt;% 
    bind_rows(tibble(alpha, beta))
}</code></pre>
<pre><code>## 在第 1582 次 疊代已達收斂條件</code></pre>
<pre class="r"><code># 整理學習結果
tibble(Intercept = alpha, carat = beta)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   Intercept  carat
##       &lt;dbl&gt;  &lt;dbl&gt;
## 1    -3043. 11682.</code></pre>
<p>由結果可以看出，經過1,582次的訓練後，梯度下降法得到的係數值與<code>lm</code>函數及封閉解的結果相同，訓練成功！</p>
<p>我們可以透過繪圖來了解機器學習的過程，首先需先繪製出誤差函數的等高線圖，再來將學習路徑標示在圖上。</p>
<pre class="r"><code># 計算不同截距項及係數項組合下的誤差函數值
plotData &lt;- NULL
for(alpha in seq(-6000, 3000, 500)){
  for(beta in seq(0, 24000, 500)){
    
    # 計算均方誤差
    yPred &lt;- alpha+ beta %*% t(x)
    mse &lt;- sum((y-yPred)^2) / length(y)
    
    # 紀錄資訊
    plotData &lt;- plotData %&gt;% bind_rows(tibble(alpha, beta, mse))
  }
}

# 繪製誤差函數的等高線圖形 並加入學習過程
ggplot(plotData, aes(x = beta, y = alpha, z = mse)) +
  # 繪製等高線圖
  stat_contour(aes(colour = ..level..), size = 1.3, bins = 20) +
  # 修改等高線顏色
  scale_colour_gradientn(colours = rainbow(20), name = &quot;均方誤差&quot;) +
  # 修改x軸名稱
  xlab(&quot;beta&quot;) +
  # 修改y軸名稱
  ylab(&quot;alpha&quot;) +
  # 標註梯度疊代每次的點位
  geom_point(data = traningRecord, 
             mapping = aes(x = beta, y = alpha), 
             size = 1.3, shape = 21, fill = &quot;white&quot;)</code></pre>
<p><img src="/post/2018-09-19-linear-gradient-descent-method_files/figure-html/learning%20graph-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>由圖可以看出，電腦從初始值(0,0)的地方，透過梯度下降法，一路收斂到均方誤差最小的地方。</p>
<p>學習比率是梯度下降法中重要的參數，若設定太小，收斂速度會太慢。但若設定值太大，則會有梯度爆炸(Gradient explosion)的狀況出現，係數值會發散而無法收斂。</p>
<p><img src="1.png" /></p>
<p>學習比率的設定依各個資料集的狀況而有所不同，在此範例中，若學習比率設定為0.9，就會發生梯度爆炸的狀況，如下圖：</p>
<p><img src="/post/2018-09-19-linear-gradient-descent-method_files/figure-html/gradient%20explosion-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>

    </div>
    
      
        <div class="pagination">
          <div class="pagination__title">
            <span class="pagination__title-h"
              >Read other posts</span
            >
            <hr />
          </div>
          <div class="pagination__buttons">
            
              <span class="button previous">
                <a href="/post/linux-ubuntu-configure/">
                  <span class="button__icon">←</span>
                  <span class="button__text">Linux(Ubuntu 18.04)建立R及GPU環境相關設定流程</span>
                </a>
              </span>
            
            
              <span class="button next">
                <a href="/post/azure-install-centos/">
                  <span class="button__text">在Azure雲端平台建立CentOS虛擬機器，並安裝遠端桌面服務、R軟體及MySQL資料庫</span>
                  <span class="button__icon">→</span>
                </a>
              </span>
            
          </div>
        </div>
      
    
    <hr>
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "suyenting" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    
      
        

      
    
  </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <a
  href="/"
  class="logo"
  style="text-decoration: none;"
>
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367" />
</svg>
</span>
    <span class="logo__text"
      >Hello Friend, I&#39;m Yestin!</span
    >
    <span class="logo__cursor"></span>
  
</a>

      <div class="copyright">
        <span
          >© 2022 Powered by
          <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a></span
        >
        <span
          >Theme created by
          <a href="https://twitter.com/panr" target="_blank" rel="noopener">panr</a></span
        >
      </div>
    
  </div>
</footer>

<script src="/assets/main.js"></script>
<script src="/assets/prism.js"></script>


      
    </div>

    
  </body>
</html>
